{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Alvaro/Desktop/phage/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqFeature import FeatureLocation\n",
    "from Bio.Seq import UndefinedSequenceError\n",
    "from collections import defaultdict\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input is a genbank file, which is the only thing to be parsed. The output from engineer features is assigned a class staining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(genbank_file):\n",
    "    # Lists to hold data\n",
    "    ids = []\n",
    "    genome_lengths = []\n",
    "    gc_contents = []\n",
    "    sequences = []\n",
    "    reverse_complements = []\n",
    "    cds_numbers = []\n",
    "    positive_strands = []\n",
    "    negative_strands = []\n",
    "    coding_capacities = []\n",
    "    molecule_types = []\n",
    "    topologies = []\n",
    "    trna_counts = []\n",
    "\n",
    "    \n",
    "    # Read the GenBank file\n",
    "    for record in SeqIO.parse(genbank_file, \"genbank\"):\n",
    "        try:\n",
    "            # Attempt to access the sequence, which may raise UndefinedSequenceError\n",
    "            sequence = str(record.seq)\n",
    "            # print(record.id)\n",
    "        except UndefinedSequenceError:\n",
    "            # print(f\"Skipping record {record.id} as sequence is undefined.\")\n",
    "            continue  # Skip this record\n",
    "\n",
    "        # Calculate genome length and GC content\n",
    "        total_length = len(sequence)\n",
    "        gc_content = round((sequence.count('C') + sequence.count('G')) / total_length * 100, 3)\n",
    "\n",
    "        # Initialize counters\n",
    "        plus = 0\n",
    "        minus = 0\n",
    "        coding_count = 0\n",
    "        trna_count = 0\n",
    "        seen = set()  # Store seen barcodes\n",
    "\n",
    "        for feature in record.features:\n",
    "            start = feature.location.start\n",
    "            end = feature.location.end\n",
    "            length = len(FeatureLocation(start, end))\n",
    "            barcode = f\"{start}_{end}_{length}\"\n",
    "\n",
    "            if feature.type != 'source' and barcode not in seen:\n",
    "                coding_count += length\n",
    "                seen.add(barcode)\n",
    "\n",
    "            if feature.type == 'CDS':\n",
    "                if feature.location.strand == 1:\n",
    "                    plus += 1\n",
    "                elif feature.location.strand == -1:\n",
    "                    minus += 1\n",
    "            elif feature.type == 'tRNA':\n",
    "                trna_count += 1\n",
    "\n",
    "        \n",
    "        # Calculate total number of CDS\n",
    "        total_CDS = plus + minus\n",
    "\n",
    "        # Calculate strand usage as a percentage\n",
    "        per_plus = round((plus / total_CDS) * 100, 2) if total_CDS != 0 else 0\n",
    "        per_minus = round((minus / total_CDS) * 100, 2) if total_CDS != 0 else 0\n",
    "\n",
    "        # Calculate coding capacity as a percentage\n",
    "        coding_capacity = (coding_count / total_length) * 100\n",
    "\n",
    "        # Extract molecule_type and topology\n",
    "        molecule_type = record.annotations.get('molecule_type', 'N/A')\n",
    "        topology = record.annotations.get('topology', 'N/A')\n",
    "\n",
    "        # Append data to lists\n",
    "        ids.append(record.id)\n",
    "        genome_lengths.append(total_length)\n",
    "        gc_contents.append(gc_content)\n",
    "        sequences.append(sequence)\n",
    "        reverse_complements.append(str(sequence[::-1]))\n",
    "        cds_numbers.append(total_CDS)\n",
    "        positive_strands.append(per_plus)\n",
    "        negative_strands.append(per_minus)\n",
    "        coding_capacities.append(coding_capacity)\n",
    "        molecule_types.append(molecule_type)\n",
    "        topologies.append(topology)\n",
    "        trna_counts.append(trna_count)\n",
    "    \n",
    "    print(\"Processing the entries...\")\n",
    "    # Convert lists to pandas DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'genome_length': genome_lengths,\n",
    "        'gc_%': gc_contents,\n",
    "        'sequence': sequences,\n",
    "        'reverse_complement': reverse_complements,\n",
    "        'cds_number': cds_numbers,\n",
    "        'positive_strand_%': positive_strands,\n",
    "        'negative_strand_%': negative_strands,\n",
    "        'coding_capacity': coding_capacities,\n",
    "        'molecule_type': molecule_types,\n",
    "        'topology': topologies,\n",
    "        'trna_count': trna_counts\n",
    "    })\n",
    "\n",
    "\n",
    "    df['id'] = df['id'].str[:-2]\n",
    "\n",
    "    # Check for unexpected molecule types\n",
    "    expected_molecule_types = ['ss-DNA', 'DNA', 'RNA', 'ss-RNA']\n",
    "\n",
    "    # Check and correct 'cRNA' entries\n",
    "    cRNA_entries = df[df['molecule_type'] == 'cRNA']\n",
    "    if not cRNA_entries.empty:\n",
    "        for entry_id in cRNA_entries['id']:\n",
    "            print(f\"Info: Entry with id '{entry_id}' has molecule type 'cRNA'. Changing it to 'RNA'.\")\n",
    "        df.loc[df['molecule_type'] == 'cRNA', 'molecule_type'] = 'RNA'\n",
    "\n",
    "    # Check and correct 'cDNA' entries\n",
    "    cDNA_entries = df[df['molecule_type'] == 'cDNA']\n",
    "    if not cDNA_entries.empty:\n",
    "        for entry_id in cDNA_entries['id']:\n",
    "            print(f\"Info: Entry with id '{entry_id}' has molecule type 'cDNA'. Changing it to 'DNA'.\")\n",
    "        df.loc[df['molecule_type'] == 'cDNA', 'molecule_type'] = 'DNA'\n",
    "\n",
    "    unexpected_types = set(df['molecule_type']) - set(expected_molecule_types)\n",
    "\n",
    "    if unexpected_types:\n",
    "        for utype in unexpected_types:\n",
    "            # Get ids of entries with the unexpected molecule type\n",
    "            ids_to_exclude = df[df['molecule_type'] == utype]['id'].tolist()\n",
    "            for entry_id in ids_to_exclude:\n",
    "                print(f\"Warning: Entry with id '{entry_id}' has unrecognized molecule type '{utype}'. It will not be considered.\")\n",
    "            df = df[df['molecule_type'] != utype]\n",
    "            \n",
    "    df = pd.get_dummies(df, columns=['molecule_type'])\n",
    "\n",
    "    expected_columns = ['jumbophage', 'molecule_type_ss-DNA', 'molecule_type_DNA', 'molecule_type_RNA', 'molecule_type_ss-RNA']\n",
    "    for col in expected_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0  # Filling with zeros\n",
    "        df[col] = df[col].astype(bool)  # Convert to boolean\n",
    "\n",
    "    df['jumbophage'] = df['genome_length'].apply(lambda x: x >= 200000)\n",
    "    df['jumbophage'] = df['jumbophage'].astype(int)  # Convert True/False to 1/0\n",
    "    df = pd.get_dummies(df, columns=['topology'])\n",
    "    return df\n",
    "\n",
    "def staining_feature(staining_df, features_df):\n",
    "    stain = pd.read_csv(staining_df, index_col = 0)\n",
    "    stain = stain[['Accession', 'staining']]\n",
    "    stain = stain.rename(columns={'Accession': 'id'})\n",
    "\n",
    "    features_df = pd.merge(features_df, stain, on='id', how='left')\n",
    "    features_df = features_df[['id', 'staining', 'genome_length', 'jumbophage', 'gc_%', 'trna_count','cds_number', 'coding_capacity',\n",
    "                               'positive_strand_%','negative_strand_%', 'molecule_type_ss-DNA', 'molecule_type_DNA', 'molecule_type_RNA', 'molecule_type_ss-RNA',\n",
    "                               'topology_circular', 'topology_linear']]\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Load a trained model from a given path.\")\n",
    "    optional = parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "\n",
    "    parser.add_argument(\"-g\", \"--genbank\", type=str, required=True,\n",
    "                        help=\"Path to genbank file of the entry.\")\n",
    "    optional.add_argument(\"-s\",\"--staining\", dest=\"staining\", action='store', \n",
    "                          default='data/interim/gram_staining/staining_assignation.csv', \n",
    "                          help=\"Path to csv file with a customised gram staining class assignation for every entry.\")\n",
    "    optional.add_argument(\"-o\",\"--output_direcotry\", dest=\"output\", action='store', \n",
    "                          default='data/processed/model_data_pharokka.csv', help=\"Output path (default: data/processed/model_data_pharokka.csv).\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # The genbank file input should be the output of pharokka \n",
    "\n",
    "    print(\"Reading the genbank file...\")\n",
    "    features_df = engineer_features(args.genbank)\n",
    "\n",
    "    features_df = staining_feature(args.staining, features_df)\n",
    "\n",
    "    features_df.to_csv(args.output, index = False)\n",
    "    \n",
    "    print(f\"Processed data has been stored in {args.output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is a folder containing the pharokka output for a set of phages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(folder):\n",
    "    # Lists to hold data\n",
    "    ids = []\n",
    "    genome_lengths = []\n",
    "    gc_contents = []\n",
    "    sequences = []\n",
    "    reverse_complements = []\n",
    "    cds_numbers = []\n",
    "    positive_strands = []\n",
    "    negative_strands = []\n",
    "    coding_capacities = []\n",
    "    molecule_types = []\n",
    "    topologies = []\n",
    "    trna_counts = []\n",
    "\n",
    "    genbank_file = f'{folder}/pharokka.gbk'\n",
    "\n",
    "    # Read the GenBank file\n",
    "    for record in SeqIO.parse(genbank_file, \"genbank\"):\n",
    "        try:\n",
    "            # Attempt to access the sequence, which may raise UndefinedSequenceError\n",
    "            sequence = str(record.seq)\n",
    "            # print(record.id)\n",
    "        except UndefinedSequenceError:\n",
    "            # print(f\"Skipping record {record.id} as sequence is undefined.\")\n",
    "            continue  # Skip this record\n",
    "\n",
    "        # Calculate genome length and GC content\n",
    "        total_length = len(sequence)\n",
    "        gc_content = round((sequence.count('C') + sequence.count('G')) / total_length * 100, 3)\n",
    "\n",
    "        # Initialize counters\n",
    "        plus = 0\n",
    "        minus = 0\n",
    "        coding_count = 0\n",
    "        trna_count = 0\n",
    "        seen = set()  # Store seen barcodes\n",
    "\n",
    "        for feature in record.features:\n",
    "            start = feature.location.start\n",
    "            end = feature.location.end\n",
    "            length = len(FeatureLocation(start, end))\n",
    "            barcode = f\"{start}_{end}_{length}\"\n",
    "\n",
    "            if feature.type != 'source' and barcode not in seen:\n",
    "                coding_count += length\n",
    "                seen.add(barcode)\n",
    "\n",
    "            if feature.type == 'CDS':\n",
    "                if feature.location.strand == 1:\n",
    "                    plus += 1\n",
    "                elif feature.location.strand == -1:\n",
    "                    minus += 1\n",
    "            elif feature.type == 'tRNA':\n",
    "                trna_count += 1\n",
    "\n",
    "        \n",
    "        # Calculate total number of CDS\n",
    "        total_CDS = plus + minus\n",
    "\n",
    "        # Calculate strand usage as a percentage\n",
    "        per_plus = round((plus / total_CDS) * 100, 2) if total_CDS != 0 else 0\n",
    "        per_minus = round((minus / total_CDS) * 100, 2) if total_CDS != 0 else 0\n",
    "\n",
    "        # Calculate coding capacity as a percentage\n",
    "        coding_capacity = (coding_count / total_length) * 100\n",
    "\n",
    "        # Extract molecule_type and topology\n",
    "        molecule_type = record.annotations.get('molecule_type', 'N/A')\n",
    "        topology = record.annotations.get('topology', 'N/A')\n",
    "\n",
    "        # Append data to lists\n",
    "        ids.append(record.id)\n",
    "        genome_lengths.append(total_length)\n",
    "        gc_contents.append(gc_content)\n",
    "        sequences.append(sequence)\n",
    "        reverse_complements.append(str(sequence[::-1]))\n",
    "        cds_numbers.append(total_CDS)\n",
    "        positive_strands.append(per_plus)\n",
    "        negative_strands.append(per_minus)\n",
    "        coding_capacities.append(coding_capacity)\n",
    "        molecule_types.append(molecule_type)\n",
    "        topologies.append(topology)\n",
    "        trna_counts.append(trna_count)\n",
    "    \n",
    "    print(\"Processing the entries...\")\n",
    "    # Convert lists to pandas DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'id_inphared': ids,\n",
    "        'genome_length_inphared': genome_lengths,\n",
    "        'gc_%_inphared': gc_contents,\n",
    "        'sequence_inphared': sequences,\n",
    "        'reverse_complement_inphared': reverse_complements,\n",
    "        'cds_number_inphared': cds_numbers,\n",
    "        'positive_strand_%_inphared': positive_strands,\n",
    "        'negative_strand_%_inphared': negative_strands,\n",
    "        'coding_capacity_inphared': coding_capacities,\n",
    "        'molecule_type_inphared': molecule_types,\n",
    "        'topology_inphared': topologies\n",
    "    })\n",
    "\n",
    "\n",
    "    df['id_inphared'] = df['id_inphared'].str[:-2]\n",
    "\n",
    "    # Check for unexpected molecule types\n",
    "    expected_molecule_types = ['ss-DNA', 'DNA', 'RNA', 'ss-RNA']\n",
    "\n",
    "    # Check and correct 'cRNA' entries\n",
    "    cRNA_entries = df[df['molecule_type_inphared'] == 'cRNA']\n",
    "    if not cRNA_entries.empty:\n",
    "        for entry_id in cRNA_entries['id_inphared']:\n",
    "            print(f\"Info: Entry with id '{entry_id}' has molecule type 'cRNA'. Changing it to 'RNA'.\")\n",
    "        df.loc[df['molecule_type_inphared'] == 'cRNA', 'molecule_type_inphared'] = 'RNA'\n",
    "\n",
    "    # Check and correct 'cDNA' entries\n",
    "    cDNA_entries = df[df['molecule_type_inphared'] == 'cDNA']\n",
    "    if not cDNA_entries.empty:\n",
    "        for entry_id in cDNA_entries['id_inphared']:\n",
    "            print(f\"Info: Entry with id '{entry_id}' has molecule type 'cDNA'. Changing it to 'DNA'.\")\n",
    "        df.loc[df['molecule_type_inphared'] == 'cDNA', 'molecule_type_inphared'] = 'DNA'\n",
    "\n",
    "    unexpected_types = set(df['molecule_type_inphared']) - set(expected_molecule_types)\n",
    "\n",
    "    if unexpected_types:\n",
    "        for utype in unexpected_types:\n",
    "            # Get ids of entries with the unexpected molecule type\n",
    "            ids_to_exclude = df[df['molecule_type_inphared'] == utype]['id'].tolist()\n",
    "            for entry_id in ids_to_exclude:\n",
    "                print(f\"Warning: Entry with id '{entry_id}' has unrecognized molecule type '{utype}'. It will not be considered.\")\n",
    "            df = df[df['molecule_type'] != utype]\n",
    "            \n",
    "    df = pd.get_dummies(df, columns=['molecule_type_inphared'])\n",
    "    df = pd.get_dummies(df, columns=['topology_inphared'])\n",
    "    expected_columns = ['jumbophage_inphared', 'topology_linear_inphared','topology_circular_inphared',\n",
    "    'molecule_inphared_type_ss-DNA', 'molecule_inphared_type_DNA', 'molecule_inphared_type_RNA', 'molecule_inphared_type_ss-RNA']\n",
    "    \n",
    "    for col in expected_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0  # Filling with zeros\n",
    "        df[col] = df[col].astype(bool)  # Convert to boolean\n",
    "\n",
    "    df['jumbophage_inphared'] = df['genome_length_inphared'].apply(lambda x: x >= 200000)\n",
    "    df['jumbophage_inphared'] = df['jumbophage_inphared'].astype(int)  # Convert True/False to 1/0\n",
    "    \n",
    "    df['id_inphared'] = df['id_inphared'].str.slice(0,8)\n",
    "\n",
    "\n",
    "    df_length_gc_cds_density = pd.read_csv(f\"{folder}pharokka_length_gc_cds_density.tsv\", sep=\"\\t\")\n",
    "    df_length_gc_cds_density['contig'] = df_length_gc_cds_density['contig'].str.slice(0,8)\n",
    "    df_length_gc_cds_density\n",
    "\n",
    "    df = pd.merge(df, df_length_gc_cds_density, left_on='id_inphared', right_on='contig', how='outer')\n",
    "    del(df_length_gc_cds_density)\n",
    "\n",
    "    df_cds = pd.read_csv(f\"{folder}pharokka_cds_functions.tsv\", sep=\"\\t\")\n",
    "    \n",
    "    # Define a dictionary of replacements\n",
    "    replacements = {\n",
    "        \"DNA, RNA and nucleotide metabolism\":\"nucleotide_metabolism\",\n",
    "        \"head and packaging\": \"head_packaging\",\n",
    "        \"moron, auxiliary metabolic gene and host takeover\": \"host_takeover\",\n",
    "        \"transcription regulation\": \"transcription\",\n",
    "        \"unknown function\": \"unkown_function\"\n",
    "    }\n",
    "\n",
    "    # Replace the values using the dictionary\n",
    "    df_cds['Description'] = df_cds['Description'].replace(replacements)\n",
    "\n",
    "    df_cds = df_cds.pivot(index='contig', columns='Description', values='Count').reset_index()\n",
    "    df_cds['contig'] = df_cds['contig'].str.slice(0,8)\n",
    "\n",
    "    df = pd.merge(df, df_cds, left_on='id_inphared', right_on='contig', how='outer')\n",
    "    del(df_cds)\n",
    "\n",
    "    # Specify dtype to avoid importing issues \n",
    "    dtype_spec = {\n",
    "        'vfdb_species': 'str',\n",
    "        'CARD_eVal': 'float',  # Float because it's numeric but can contain NaN\n",
    "        'CARD_species': 'str',\n",
    "        'ARO_Accession': 'str',\n",
    "        'CARD_short_name': 'str',\n",
    "        'Protein_Accession': 'str',\n",
    "        'DNA_Accession': 'str',\n",
    "        'AMR_Gene_Family': 'str',\n",
    "        'Drug_Class': 'str'\n",
    "    }\n",
    "\n",
    "    df_frame = pd.read_csv(f\"{folder}pharokka_cds_final_merged_output.tsv\", sep=\"\\t\", dtype=dtype_spec, low_memory=False)\n",
    "\n",
    "    frame_counts = df_frame.groupby('contig')['frame'].value_counts().unstack(fill_value=0)\n",
    "    frame_counts.columns = ['frame_negative', 'frame_positive']\n",
    "    frame_counts = frame_counts.rename_axis(None, axis=1).reset_index()\n",
    "    frame_counts['contig'] = frame_counts['contig'].str.slice(0,8)\n",
    "    df = pd.merge(df, frame_counts, left_on='id_inphared', right_on='contig', how='outer')\n",
    "\n",
    "    df = df.rename(columns={'id_inphared': 'id'})\n",
    "\n",
    "    # List of columns to be ignored in the duplicate check\n",
    "    ignore_columns = {\n",
    "        'CARD_AMR_Genes', 'CDS', 'CRISPRs', 'VFDB_Virulence_Factors', 'connector',\n",
    "        'head_packaging', 'host_takeover', 'integration and excision', 'lysis',\n",
    "        'nucleotide_metabolism', 'other', 'tRNAs', 'tail', 'tmRNAs',\n",
    "        'transcription', 'unkown_function'\n",
    "    }\n",
    "\n",
    "    # # Function to check for repeated values excluding 0 and 1 and certain columns\n",
    "    # def find_repeated_values(row):\n",
    "    #     # Use defaultdict to associate multiple columns with the same value\n",
    "    #     repeated_columns = defaultdict(list)\n",
    "    #     # Use items() instead of iteritems()\n",
    "    #     for col, value in row.items():\n",
    "    #         if col in ignore_columns:  # Skip the iteration for specified columns\n",
    "    #             continue\n",
    "    #         if (value in [0, 1]) or pd.isnull(value):  # Skip values of 0, 1, or NaN\n",
    "    #             continue\n",
    "    #         if row[row == value].count() > 1:  # If value count in row is more than one, it's a duplicate\n",
    "    #             repeated_columns[value].append(col)\n",
    "    #     return repeated_columns\n",
    "\n",
    "    # # Ensure row is a Series before passing to find_repeated_values\n",
    "    # for index, row in df.iterrows():\n",
    "    #     repeated_values_dict = find_repeated_values(row)\n",
    "    #     if repeated_values_dict:  # If there's any repeated value\n",
    "    #         warning_message = f\"Entry {row['id']} shows repeated values in \"\n",
    "    #         # Create a combined message for duplicate columns sharing the same value\n",
    "    #         message_parts = []\n",
    "    #         for val, cols in repeated_values_dict.items():\n",
    "    #             if len(cols) > 1:  # Only add to message if there are indeed duplicates\n",
    "    #                 combined_cols = ' and '.join(cols)  # Combine column names\n",
    "    #                 message_parts.append(f\"columns {combined_cols} (value {val})\")\n",
    "    #         warning_message += ', '.join(message_parts)\n",
    "    #         print(warning_message)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staining_feature(staining_df, features_df):\n",
    "    stain = pd.read_csv(staining_df, index_col = 0)\n",
    "    stain = stain[['Accession', 'staining']]\n",
    "    stain = stain.rename(columns={'Accession': 'id'})\n",
    "\n",
    "    features_df = pd.merge(features_df, stain, on='id', how='left')\n",
    "    features_df = features_df[['id', 'staining', 'genome_length_inphared', 'gc_%_inphared',\n",
    "        'cds_number_inphared', 'positive_strand_%_inphared',\n",
    "        'negative_strand_%_inphared', 'coding_capacity_inphared',\n",
    "        'molecule_type_inphared_DNA', 'topology_inphared_linear',\n",
    "        'jumbophage_inphared', 'topology_linear_inphared',\n",
    "        'topology_circular_inphared', 'molecule_inphared_type_ss-DNA',\n",
    "        'molecule_inphared_type_DNA', 'molecule_inphared_type_RNA',\n",
    "        'molecule_inphared_type_ss-RNA', 'length', 'gc_perc',\n",
    "        'transl_table', 'cds_coding_density', 'CARD_AMR_Genes',\n",
    "        'CDS', 'CRISPRs', 'VFDB_Virulence_Factors', 'connector',\n",
    "        'head_packaging', 'host_takeover', 'integration and excision', 'lysis',\n",
    "        'nucleotide_metabolism', 'other', 'tRNAs', 'tail', 'tmRNAs',\n",
    "        'transcription', 'unkown_function', 'frame_negative',\n",
    "        'frame_positive', 'sequence_inphared', 'reverse_complement_inphared']]\n",
    "\n",
    "\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the entries...\n"
     ]
    }
   ],
   "source": [
    "code = \"MZ079855__Klebsiella_phage_vB_Kpn_3\" \n",
    "code = \"MW006474__Salmonella_phage_GEC_vB_B1\"\n",
    "entry = f\"../data/interim/pharokka/tmp_pharokka/{code}.fna_pharokka/\"\n",
    "entry = \"../data/interim/pharokka/output_50/\"\n",
    "df = engineer_features(entry)\n",
    "stain = \"../data/interim/gram_staining/staining_assignation.csv\"\n",
    "df = staining_feature(stain,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Process raw data into features to train a model.\")\n",
    "    optional = parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "    parser.add_argument(\"-d\", \"--data\", type=str, required=True,\n",
    "                        help=\"Path to the data. Accepted inputs are FASTA files for Pharokka inputs or a directory path to an already processed Pharokka output.\\\n",
    "                            If the input is a FASTA file, the Pharokka output will be stored locally.\")\n",
    "    optional.add_argument(\"-s\",\"--staining\", dest=\"staining\", action='store', \n",
    "                          default='data/interim/gram_staining/staining_assignation.csv', \n",
    "                          help=\"Path to csv file with a customised gram staining class assignation for every entry.\")\n",
    "    optional.add_argument(\"-o\",\"--output_directory\", dest=\"output\", action='store', \n",
    "                          default='data/processed/model_data_pharokka.csv', help=\"Output path (default: data/processed/model_data_pharokka.csv).\")\n",
    "    optional.add_argument(\"-p\",\"--pharokka_directory\", dest=\"pharokka\", action='store', \n",
    "                          default=\"/mnt/c/Users/Alvaro/Desktop/pharokka_output/\", help=\"Path where pharokka output will be stored (default: /mnt/c/Users/Alvaro/Desktop/pharokka_output/).\")\n",
    "    optional.add_argument(\"-b\",\"--database_directory\", dest=\"database\", action='store', \n",
    "                          default=\"/mnt/c/Users/Alvaro/Desktop/pharokka_database/\", help=\"Path to the Pharokka database (default./mnt/c/Users/Alvaro/Desktop/pharokka_database/).\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    '''\n",
    "    Possible inputs:\n",
    "    - Pharokka output\n",
    "    - Fasta file on which to build pharokka output\n",
    "\n",
    "    This output will be the input for the engineer features and staining feature, and its\n",
    "    output is stored in args.output. This output is used by train_model for training\n",
    "    '''\n",
    "    fasta_extensions = [\"fasta\", \"fas\", \"fa\", \"fna\", \"ffn\", \"faa\", \"mpfa\", \"frn\"]\n",
    "\n",
    "    def unix_call(command, environment=None):\n",
    "        if environment:  # If the environment is specified, prepend with 'conda run -n env_name'\n",
    "            command = f\"conda run -n {environment} {command}\"\n",
    "        subprocess.run(command, shell=True, check=True)  \n",
    "\n",
    "    def process_fasta(file_path, output_dir, database_dir):\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Processing FASTA file: {file_path}\")\n",
    "\n",
    "        # Run Pharokka using the pharokkaENV environment\n",
    "        unix_call(f'pharokka.py -i {file_path} -o {output_dir} -f -m -t 16 -d {database_dir}', environment='pharokkaENV')\n",
    "\n",
    "        \n",
    "    # Check if the input data path is a directory or a file\n",
    "    if os.path.isdir(args.data):\n",
    "        # The input is a directory; process all relevant files within\n",
    "        print(f\"Processing Pharokka output in directory: {args.data}\")\n",
    "        model_data = engineer_features(args.data)\n",
    "\n",
    "    elif os.path.isfile(args.data):\n",
    "        # The input is a file; check if it's a FASTA file\n",
    "        file_ext = os.path.splitext(args.data)[1][1:].lower()\n",
    "        if file_ext in fasta_extensions:\n",
    "            process_fasta(args.data, args.pharokka, args.database) \n",
    "\n",
    "            # The input is a directory; process all relevant files within\n",
    "            print(f\"Processing Pharokka output in directory: {args.pharokka}\")\n",
    "            model_data = engineer_features(args.pharokka)\n",
    "\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_ext}. Supported FASTA formats are: {', '.join(fasta_extensions)}\")\n",
    "            exit(1)\n",
    "\n",
    "\n",
    "    model_data = staining_feature(args.staining, model_data)\n",
    "\n",
    "    model_data.to_csv(args.output, index = False)\n",
    "    \n",
    "    print(f\"Processed data has been stored in {args.output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative_strand_%_inphared</th>\n",
       "      <th>frame_positive</th>\n",
       "      <th>positive_strand_%_inphared</th>\n",
       "      <th>frame_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66.0</td>\n",
       "      <td>66</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative_strand_%_inphared  frame_positive  positive_strand_%_inphared  \\\n",
       "2                        66.0              66                        34.0   \n",
       "\n",
       "   frame_negative  \n",
       "2              34  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weird issue: Entry AY129332 shows repeated values in columns positive_strand_%_inphared and frame_negative (value 34.0), columns negative_strand_%_inphared and frame_positive (value 66.0)\n",
    "\n",
    "df[df[\"id_inphared\"] == \"AY129332\"][[\"negative_strand_%_inphared\", \"frame_positive\", \"positive_strand_%_inphared\", \"frame_negative\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version of engineer features\n",
    "\n",
    "\n",
    "# Extract the file extension\n",
    "file_ext = os.path.splitext(args.data)[1][1:] \n",
    "\n",
    "fasta_extensions = [\"fasta\", \"fas\", \"fa\", \"fna\", \"ffn\", \"faa\", \"mpfa\", \"frn\"]\n",
    "genbank_extensions = [\"gb\", \"gbk\"]\n",
    "\n",
    "# Processing the input data --------\n",
    "\n",
    "#TODO: pharokka\n",
    "# Process as a FASTA file\n",
    "if file_ext in fasta_extensions:\n",
    "    pass  # Replace PHAROKKA\n",
    "\n",
    " # Process as a GenBank file\n",
    "elif file_ext in genbank_extensions:\n",
    "    model_data = engineer_features(args.data)\n",
    "\n",
    "else:\n",
    "    supported_formats = ', '.join(fasta_extensions + genbank_extensions)\n",
    "    print(f\"Unsupported file format: {file_ext}. Supported formats are: {supported_formats}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "features = ['genome_length', 'jumbophage', 'gc_%',\n",
    "       'trna_count', 'cds_number', 'coding_capacity', 'positive_strand_%',\n",
    "       'negative_strand_%', 'molecule_type_ss-DNA', 'molecule_type_DNA',\n",
    "       'molecule_type_RNA', 'molecule_type_ss-RNA', 'topology_circular','topology_linear']\n",
    "model_data = model_data.set_index(model_data.columns[0])\n",
    "model_data = model_data[features]\n",
    "\n",
    "# Making the prediction -----\n",
    "\n",
    "# Load the model using the provided path\n",
    "rf = pickle.load(open(args.model, \"rb\"))\n",
    "\n",
    "# Predictions\n",
    "new_data_pred = rf.predict(model_data)\n",
    "\n",
    "# Get the probabilities for the predicted class for each instance\n",
    "probas = rf.predict_proba(model_data)\n",
    "predicted_indices = np.argmax(probas, axis=1)  # Get index of max proba for each sample\n",
    "new_data_pred_proba = [probas[i][predicted_indices[i]] for i in range(len(predicted_indices))]\n",
    "\n",
    "# Prepare the output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'id': model_data.index,\n",
    "    'prediction': new_data_pred,\n",
    "    'prediction_probability': new_data_pred_proba\n",
    "})\n",
    "\n",
    "# Uncomment the following line if you want to save the output to a CSV\n",
    "output_df.to_csv(args.output, index=False)\n",
    "\n",
    "print()\n",
    "print(f\"The output has been saved in {args.output}. The first 5 entries are: \")\n",
    "print(output_df.head())\n",
    "\n",
    "# python -m src.models.predict_model -d data/interim/genbank_engineering/50_sequences.gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def unix_call(command, environment=None):\n",
    "    if environment:  # If the environment is specified, prepend with 'conda run -n env_name'\n",
    "        command = f\"conda run -n {environment} {command}\"\n",
    "    subprocess.run(command, shell=True, check=True)  # Use 'shell=True' to interpret the command as a shell command\n",
    "\n",
    "def process_fasta(file_path, output_dir, database_dir):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Processing FASTA file: {file_path}\")\n",
    "\n",
    "    # Run Pharokka using the pharokkaENV environment\n",
    "    unix_call(f'pharokka.py -i {file_path} -o {output_dir} -f -m -t 16 -d {database_dir}', environment='pharokkaENV')\n",
    "\n",
    "    \n",
    "# Check if the input data path is a directory or a file\n",
    "if os.path.isdir(args.data):\n",
    "    # The input is a directory; process all relevant files within\n",
    "    print(f\"Processing Pharokka output in directory: {args.data}\")\n",
    "    model_data = engineer_features(args.data)\n",
    "\n",
    "elif os.path.isfile(args.data):\n",
    "    # The input is a file; check if it's a FASTA file\n",
    "    file_ext = os.path.splitext(args.data)[1][1:].lower()\n",
    "    if file_ext in fasta_extensions:\n",
    "        output_dir = \"/mnt/c/Users/Alvaro/Desktop/pharokka_output/\"\n",
    "        database_dir = \"/mnt/c/Users/Alvaro/Desktop/pharokka_database/\"\n",
    "        # TODO: add argument for output directory, databse directory and threads for pharokka\n",
    "        process_fasta(args.data, output_dir, database_dir)  # Call your processing function\n",
    "\n",
    "        # The input is a directory; process all relevant files within\n",
    "        print(f\"Processing Pharokka output in directory: {output_dir}\")\n",
    "        model_data = engineer_features(output_dir)\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_ext}. Supported FASTA formats are: {', '.join(fasta_extensions)}\")\n",
    "        exit(1)\n",
    "\n",
    "else:\n",
    "    print(f\"The provided data path does not exist: {args.data}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "features = ['genome_length_inphared', 'gc_%_inphared',\n",
    "        'cds_number_inphared', 'positive_strand_%_inphared',\n",
    "        'negative_strand_%_inphared', 'coding_capacity_inphared',\n",
    "        'molecule_type_inphared_DNA', 'topology_inphared_linear',\n",
    "        'jumbophage_inphared', 'topology_linear_inphared',\n",
    "        'topology_circular_inphared', 'molecule_inphared_type_ss-DNA',\n",
    "        'molecule_inphared_type_DNA', 'molecule_inphared_type_RNA',\n",
    "        'molecule_inphared_type_ss-RNA', 'length', 'gc_perc',\n",
    "        'transl_table', 'cds_coding_density', 'CARD_AMR_Genes',\n",
    "        'CDS', 'CRISPRs', 'VFDB_Virulence_Factors', 'connector',\n",
    "        'head_packaging', 'host_takeover', 'integration and excision', 'lysis',\n",
    "        'nucleotide_metabolism', 'other', 'tRNAs', 'tail', 'tmRNAs',\n",
    "        'transcription', 'unkown_function', 'frame_negative',\n",
    "        'frame_positive', 'sequence_inphared', 'reverse_complement_inphared']\n",
    "\n",
    "model_data = model_data.set_index(model_data.columns[0])\n",
    "\n",
    "\n",
    "model_data = model_data[features]\n",
    "\n",
    "\n",
    "# Making the prediction -----\n",
    "\n",
    "# Load the model using the provided path\n",
    "rf = pickle.load(open(args.model, \"rb\"))\n",
    "\n",
    "# Predictions\n",
    "new_data_pred = rf.predict(model_data)\n",
    "\n",
    "# Get the probabilities for the predicted class for each instance\n",
    "probas = rf.predict_proba(model_data)\n",
    "predicted_indices = np.argmax(probas, axis=1)  # Get index of max proba for each sample\n",
    "new_data_pred_proba = [probas[i][predicted_indices[i]] for i in range(len(predicted_indices))]\n",
    "\n",
    "# Prepare the output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'id': model_data.index,\n",
    "    'prediction': new_data_pred,\n",
    "    'prediction_probability': new_data_pred_proba\n",
    "})\n",
    "\n",
    "# Uncomment the following line if you want to save the output to a CSV\n",
    "output_df.to_csv(args.output, index=False)\n",
    "\n",
    "print()\n",
    "print(f\"The output has been saved in {args.output}. The first 5 entries are: \")\n",
    "print(output_df.head())\n",
    "\n",
    "# python -m src.models.predict_model -d data/interim/genbank_engineering/50_sequences.gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
